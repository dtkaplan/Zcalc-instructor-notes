---
title: "Linear Algebra in Introductory Calculus"
description: |
  A new article created using the Distill format.
author:
  - name: Daniel T. Kaplan
    affiliation: Macalester College and US Air Force Academy
date: "`r Sys.Date()`"
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(Zcalc)
```

::: {.hidden}
$$
\let\origvec\vec
\let\origmathit\mathit
\let\orighat\hat
\let\origbar\bar
\newcommand{\vec}[1]{\overset{{\rule[-1pt]{0mm}{1mm}}\rightharpoonup}{\mathbf{#1}}}
\newcommand{\bar}[1]{\overset{{\rule[-1pt]{12pt}{.5mm}}}{\mathbf{#1}}}
\newcommand{\mathit}[1]{\underset{\leftharpoondown}{\overset{{\rightharpoonup}}{\large\mathbf #1}}}
\newcommand{\hat}[1]{\widehat{\ \mathbf#1\ }}
\newcommand{\len}[1]{{\|{\mathbf #1}\|}} 
$$
:::

The use of mathematics in the worlds of science, management, and the economy has increased dramatically in the last half century. Much of this relates to the collection and interpretation of data, now routinely performed at a scale unanticipated even 25 years ago. Statistics, data science, and machine learning are now an part of the professional toolkit for workers in the information economy.

This paper discusses an opportunity, via linear algebra, for the university-level mathematics curriculum to forge genuine connections to statistics and data science and the foundations of machine learning.

The substantial change in the university-level mathematical sciences curriculum in the last 50 years is the ubiquitous offering of introductory statistics courses. Statistics has practically become part of general education at the university level and is the technical subject required by the widest range of fields.

The flavor of statistics most widely taught has little or no connection to the historical mainspring of university mathematics: calculus. Indeed, statistics is often seen as an *alternative* to calculus or pre-calculus. For students in many fields such as the health or social sciences, where requirements for calculus and its pre-requisites form a major, unneeded barrier to entry into professional work, statistics as an alternative to calculus makes good sense.

The disconnect between statistics and calculus is unfortunate in other ways (Kaplan 2013). One problem is that the conventional introductory statistics course is based on arithmetic procedures from the late 1800s and early 1900s and therefore fails to provide the concepts and methods needed for large scale work with data in realistically complex settings. Machine learning and Bayesian statistics are virtually inaccessible to graduates of introductory statistics.

Another unfortunate consequence of the separation of statistics and calculus is that those students whose work necessitates a solid introduction to calculus cannot apply the concepts and skills that they learn to their statistics studies; introductory statistics courses are not set up to draw on appropriate topics from calculus.

The same is true going the other way. There is little or no content in the first three or four semesters of a conventional calculus curriculum designed to foster statistical thinking. It seems likely this is due to the calculus curriculum being set before the advent of widespread professional use of data and the misconception among many mathematicians that formal probability is at the core of statistics. (In fact, simple notions of randomness coupled to modern computing are an effective way of introducing statistics. See LOCK 5, George Cobb Ptolemaic curriculum.)

A major opportunity for substantial connections between calculus and statistics arises in what is usually a fourth- or fifth-semester course: linear algebra. In practice, this opportunity is lost due to two problems: 1) only a very small fraction of students starting in Calc I will ever reach linear algebra; and 2) the topics selected for the mainstream linear algebra curriculum focus on issues such as Gaussian elimination, the invertibility of matrices, and determinants that are largely irrelevant to statistics.

Neither of the above two problems is an inevitable consequence of the nature of calculus or of linear algebra. The modeling-based calculus, statistics, and computing curriculum promoted by Project MOSAIC is one attempt to circumvent such problems. The statistics arm of Project MOSAIC centers on modeling, randomization, and accessible high-level computing. The calculus arm of the project focuses on a short, accessible, and comprehensive introduction to calculus. Here, "comprehensive" refers to including functions of multiple variables, differential equations, and linear algebra in the first one or two semesters of calculus.

This paper describes how concepts and methods of linear algebra well suited to professional work with data are presented in the MOSAIC calculus curriculum. 

[Austin's idea: connections of calculus to "Statistical Rethinking"]

## Calculus background to linear algebra

A student who completes the traditional first two semesters of calculus, browsing the ample shelf of linear algebra texts, would be hard pressed to see any connection of their calculus studies to what is found in the linear algebra textbooks. There are exceptions, but a prevalent way of introducing linear algebra often starts with systems of linear equations and how to solve them. A person unaware of the manifold uses of linear algebra might see the subject as an extension of high-school algebra, where the solution of two linear equations in two unknowns is a standard topic.

Why linear algebra should be postponed to late in the calculus sequence is, to me, something of a mystery. Some possible reasons: calculus is seen as a topic of higher priority; vectors are typically introduced in third-semester calculus; linear algebra is, in some curricular organizations, allied with differential equations, which typically comes late in the calculus sequence.

In the MOSAIC curriculum, calculus is taught from a modeling perspective. This means that students are, from the beginning, learning fundamentals of mathematical modeling. This includes a basic set of functions, e.g. exponential, power-law, log, sinusoidal, etc. and ways of putting the basic functions together to form models. The three main ways of combining functions are composition, multiplication, and linear combination. The term "linear combination" comes early in the MOSAIC curriculum. Students first encounter it in the selection of parameters in models of simple oscillatory phenomena ($A + B \sin(2 \pi \omega t)$) or exponential decay ($A + B e^{-kt}$) in choosing $A$ and $B$ to match the model function to data.

Another important theme in the MOSAIC curriculum is using low-order polynomial approximations---including in two or more variables---as a modeling framework. For instance, in the case of a function with two inputs, the modeler's task is to choose which terms to include from among the following:

$$f(x, y) \equiv \underbrace{a_0 + a_x x + a_y y}_{\text{almost always}} + \underbrace{a_{xy} x y}_{\text{interaction}} + \underbrace{a_{xx} x^2 + a_{yy} y^2}_{\text{quadratic terms}}$$
This is, of course, a linear combination of the functions $1, x, y, xy, x^2,$ and $y^2$.

Students learn to incorporate an interaction term if the effect of $x$ on the output depends on $y$, and to include one or both quadratic terms only if the phenomenon being modeled involves an extremum. Once such decisions have been made, the task of modeling is to infer the signs and magnitudes of the coefficients in the linear combination. This can be done to correspond with an understanding of the mechanism of the system, or to match the model to example cases with inputs and outputs recorded.

Since MOSAIC calculus involves functions with multiple inputs early, the gradient vector also appears, to understand the "shape" of such functions and to perform routine tasks such as optimization. (Many students will also see vectors in physics or engineering courses that are co-requisites for calculus.)

As a start, students pick the coefficients on linear combinations by eye and then through software. By the time the linear algebra component of the MOSAIC curriculum begins, students have been working extensively with linear combinations and vectors. The problem of finding coefficients for linear combinations becomes the chief motivation for introducing linear algebra. 

Later in the MOSAIC course, students encounter dynamics/differential equations where the analysis of fixed points is a central topic. Such dynamics involve characteristic behavior that can be understood in terms of eigenvectors and eigenvalues.

## The target problem

To judge from most textbooks in the field, there is a set of canonical topics covered by a  linear algebra course: matrices as a rectangular array of numbers, the solution of sets of linear equations by row operations, matrix multiplication, matrix inverses, determinants, eigenvalues and eigenvectors. In the MOSAIC curriculum, we did not seek to cover topics, but rather to provide the concepts and apparatus needed to solve what we call the "target problem."

A good setting to describe the target problem is statistical modeling. The MOSAIC calculus curriculum is strongly grounded in mathematical modeling and an important class of modeling situations involves constructing from data a model of one variable as a linear combination of other variables. To illustrate, consider data on the properties of internal combustion engines
such as that provided by McMahon and Bonner (1983) available in the `engines.csv` file at <http://www.mosaic-web.org/go/datasets/engines.csv>. Thirty-nine different engines are represented with properties such as mass, stroke length, piston bore, displacement, rpm, and power output. Constructing models relating these properties to each other, such as displacement as a function of stroke length and piston bore reveals how such engines work. 

```{r echo=FALSE}
set.seed(101)
Znotes::and_so_on(Engines[sample(1:39),]) %>% kableExtra::kable_styling()
```

Much of any modeling problem is outside the scope of linear algebra, e.g. choice of the explanatory variables and possible nonlinear transformations of those variables. Once these choices have been made, construction of a linear model is a matter of computation, using linear algebra techniques. (Usually there is a cycle of modeling, where interpretation of one model leads to a reconsideration of the variable choices, etc.) 

For instance, a (naive) model of displacement as a function of bore and stroke might involve a linear algebra problem like this one, to be solved for $\vec{x}$:

$$\underbrace{\left[\begin{array}{rrr}1 & 15.9 & 14\\
1 & 239.0 & 254\\
1 & 100.0 & 87\\
\vdots & \vdots & \vdots\\
1 & 133.0 & 127\\
1 & 105.0 & 129
\end{array}\right]}_{\text{model matrix}} 
\left[\begin{array}{c}x_1\\ x_2\\x_3\end{array}\right] \approx 
\underbrace{\left[\begin{array}{r}20\\183000\\2800\\\vdots\\12400\\7200\end{array}\right]}_{\text{`target'}}\ .$$
[Aside: One modeling skill, covered earlier in the MOSAIC curriculum, is to consider using a logarithmic transform of the variables. That would be very appropriate here.]

The "target problem" is to find the linear combination of the vectors in the model matrix to get as close as possible to the target vector. A more conventional name for this is "least squares."

In a conventional linear algebra course, least squares is inaccessible until many other topics have been mastered: matrix multiplication, transposes, inversion, and so on. In the MOSAIC curriculum, roughly a sixth of the two-semester sequence is available for linear algebra, meaning that it's hardly possible to follow the conventional sequence and get to least squares. On the other hand, the conventional topics are not needed to solve the problem and to gain a good theoretical understanding of the *relevant* properties of the solution. 

## MOSAIC topics

In choosing topics for the linear algebra component of the MOSAIC curriculum, we sought to find the most compact set of ideas that give a coherent and meaningful explanation of the target problem. (Eigenvalues and eigenvectors are not part of that compact set, but they are introduced later in the dynamics part of the MOSAIC curriculum and will be covered in a later section of this paper.) 

A full presentation of the MOSAIC topics is available through the online textbook, available at <http://www.mosaic-web.org/MOSAIC-Calculus/block-5/vectors.html>. A brief listing will suffice for our purposes here:

- Chapter 41: Vectors: length and direction, vectors in n-dimensional space, angle between vectors, orthogonality, arithmetic calculation of length and angle, the dot product.
- Chapter 42: Linear combinations of vectors: vector addition, subtraction, scaling, linear combination, matrices as collections of vectors, collections of vectors defining a subspace.
- Chapter 43: Projection and residual: projection onto a single vector, projection onto a set of vectors (particularly a set of orthogonal vectors), construction of an orthogonal basis for a subspace, construction of the model vector and the residual vector.
- Chapter 44: The target problem: finding the coefficients of the linear combination of the vectors in the model matrix to reach the projection of the target onto the model subspace.
- Chapter 45: Statistical modeling and R^2^: translating the target problem into the format and terminology used in statistical modeling.
- Chapter 46: Functions as vectors: generalizing the dot product so that functions can be treated as vectors, the Fourier basis.

## MOSAIC conventions

In developing a student's understanding of projection and the target problem, it helps to reduce the cognitive load imposed by notation and nomenclature. To this end, we use certain conventions.

Since this paper is oriented to instructors who are familiar with the material covered in Chapters 41 to 45, we'll work backwards from the way vector and matrix notation is presented to students.

- The ***target problem*** is "find the coefficients $\vec{x}$ such that $\mathit{A}\,\vec{x} = \vec{b}$" where $=$ is understood to mean "as close as possible."
- The ***model matrix*** is $\mathit{A}$ and contains the vectors that will be included in the linear approximation. Some will find heavy handed the double-arrow annotation used to identify matrices, but we want constantly to remind students that a matrix is a collection of vectors.
- The ***model subspace*** is the subspace spanned by $\mathit{A}$.
- The ***target vector*** is $\vec{b}$.
- The coefficients for the linear combination are contained in $\vec{x}$, which we find by solving the target problem.
- The ***model vector***, written $\hat{b}$, is the projection of $\vec{b}$ onto the model subspace. 
- The ***residual vector***, $\vec{r}$, is the component of $\vec{b}$ orthogonal to the model subspace. More simply, $\vec{r} = \vec{b} - \hat{b}$.

A key strategy of the course is to present most every object and concept in four modes:

1. **Graphically**. A vector is an arrow of a specified length and direction. As needed, we use three dimensional graphics where the student can interactively vary the orientation of the graphic. 
2. **Metaphorically**. A vector is a **step** in the corresponding direction and of the corresponding length. A linear combination is a trip in which steps are taken successively in the directions of the vectors being combined. The scalar coefficients of the linear combination is the number of steps in each of the directions.
3. **Arithmetically**. A vector is a **column** of numbers. (We don't introduce row vectors at all.) Basic arithmetic operations on a single vector are scaling and length. With two vectors, the basic operations are addition, subtraction, and the dot product.
4. **Computationally**. There are, of course, many suitable choices for the computing environment, such as MATLAB, Mathematica, or Maple. We think R has important advantages and that is what we use. We'll say more about computation below.


### Graphical mode

A pitfall of graphical presentations is that students can (understandably) confuse a vector with a coordinate point. We emphasize the rootlessness of vectors from the very beginning. You can place a vector wherever best suits your needs. For vector addition, the easy placement is head to tail while for subtraction and the included angle, tail to tail. To reinforce the point, early problems involve vectors drawn in arbitrary positions where the student's first step is to move the vectors into an appropriate position to carry out the operation visually. For the most part, the graphics do not include a coordinate grid. (The exception is the very narrow topic of converting between the graphical and arithmetic depictions of vectors.)

To illustrate, here is a simple exercise based in the graphics domain:

Consider the vectors drawn below: 
```{r echo=FALSE, out.width="3.5in"}
knitr::include_graphics("www/many-vectors1.png")
```
a. Remembering that mathematical vectors have only two properties---length and direction---how many different mathematical vectors are being shown. (Answer: Five. Color is not a property of a mathematical vector. Orientation, however, is.)

b. Measure the length of each vector. (Hint: Use a ruler! You can round to the nearest millimeter.)

c. Find the included angle between the $\color{blue}{\text{blue}}$ and $\color{brown}{\text{brown}}$ vectors. (Your answer should be correct to within $\pm 15^\circ$.) (Answer: About 135 degrees. Note that the orientation makes a difference. The angle is measured with the vectors placed tail to tail.)

d. Find the included angle between the $\color{magenta}{\text{magenta}}$ and $\color{blue}{\text{blue}}$ vectors. (Answer: Magenta points in the same direction as brown, so the answer is the same as in 
the previous question.)

e. Find the included angle between 0.7 times the $\color{blue}{\text{blue}}$ vector and -11.3 times the $\color{brown}{\text{brown}}$. (Answer: The scaling doesn't matter except that the negative sign for the brown vector means that its orientation will be reversed. This means that the included angle will be 45 degrees, not 135 degrees as in the previous two questions.)

-----

As another example of the graphics mode of presentation, 
the target problem with $\mathit{A} = \left[\vec{u}\ \ \vec{v}\right]$ looks like this: 

[![](target-diagram.png)](target-diagram.html)

Click on the image to open an interactive version of the diagram.

### Metaphorical mode



### Arithmetic mode


### Computational mode



## Projection, not inversion

To summarize for the knowledgeable instructor how we solve the target problem: We use QR decomposition. We present an algorithm for the construction of Q but do not cover the construction of R. This would be feasible graphically and arithmetically for subspaces defined by two vectors, but such a demonstration did not have a high priority for classroom time. Instead, we rely on software for the calculation of the coefficients $\vec{x}$ in the target problem.

To highlight the differences between the approach in the MOSAIC curriculum from a conventional curriculum, let's consider first what a conventional presentation of the target problem might look like.

1. We have a matrix $\mathbf{A}$ with more rows than columns and we seek to find $\mathbf{A} \vec{x} = \vec{b}$. Because $\mathbf{A}$ is not square, it does not have an inverse and so there is in general no solution. 

2. We can transform the problem into one that can be solved by multiplying both sides of $\mathbf{A} \vec{x} = \vec{b}$ by $\mathbf{A}^T$, getting
$$\mathbf{A}^T \mathbf{A}\, \vec{x} = \mathbf{A}^T\, \vec{b}$$
3. The matrix $\mathbf{A}^T \mathbf{A}$ is square and might therefore have an inverse. If so, we can invert it to find $\vec{x}$:
$$\vec{x} = (\mathbf{A}^T \mathbf{A})^{-1}\mathbf{A}^T\, \vec{b}$$
4. We call $(\mathbf{A}^T \mathbf{A})^{-1}\mathbf{A}^T$ the *pseudo-inverse of $\mathbf{A}$*.

Following this argument requires considerable background knowledge. We'll use *italics* to highlight those elements that are not covered in the MOSAIC curriculum. 

Step (1) calls for knowledge about *inverses* and non-square matrices. (The "in general" caveat is somewhat mysterious as stated but could be explained by constructing a non-square $\mathbf{A}$ and corresponding $\vec{b}$ that does have a solution $\vec{x}$, for instance a matrix consisting of two copies of an invertible square matrix stacked on top of one another.)

Step (2) uses the ideas of a *transpose* and *matrix-by-matrix multiplication*.

Step (3) raises the question of *when a square matrix does not have an inverse*.

Step (4) unnecessarily introduces new nomenclature (*pseudo-inverse*) and disparages the process as ungenuine.

We do not claim that the *italicized topics* listed above are not important to a thorough mastery of linear algebra. Our claim that they are not needed to solve the target problem. In addition, the construction of the pseudo-inverse is somewhat disingenuous since this is not the method used by modern software to solve the target problem. (For $\mathbf{A}$ with many columns, the calculation is ill-conditioned and fails to recognize legitimate solutions even when $(\mathbf{A}^T \mathbf{A})^{-1}$ does not exist.



## Eigenvalues and eigenvectors


## Discussion

There are linear algebra texts that do not follow the CCC. For example, Anton & Busby (2003) identify the fundamental concepts as "R^n^, orthogonality, linear combinations, spanning, subspaces, linear independence, and dimension." These are all encountered in the first chapter of their book.

The MOSAIC curriculum is far from alone in this column-centric approach to matrices. For example, the Lay, Lay, and McDonald textbook (2016) describes a "modern approach" to matrices, writing, "Good notation is crucial, and the text reflects the way scientists and engineers actually use linear algebra in practice. The definitions and proofs focus on the columns of a matrix rather than on the matrix entries. A central theme is to view a matrix–vector product Ax as a linear combination of the columns of A. This modern approach simplifies many arguments, and it ties vector space ideas into the study of linear systems."

In the CCC, where a matrix is presented as a rectangular array of numbers, the product of a matrix and a vector is often described this way:
$$\left[\begin{array}{cccc}a_{11} & a_{12} & \cdots & a_{1n}\\
\vdots &  \vdots & \vdots & \vdots\\
a_{m1} & a_{m2} & \cdots & a_{mn}\end{array}\right] \left[\begin{array}{c}x_1\\\vdots\\x_n\end{array}\right] = \left[\begin{array}{c}
a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n\\
\vdots  \\
a_{n1} x_1 + a_{n2} x_2 + \cdots + a_{mn} x_n\\
\end{array}\right]\ .$$

In the column-oriented MOSAIC curriculum, it's hardly ever necessary to write out a rectangular array of numbers. Instead, we typically write matrix-vector multiplication like this:
$$\left[\begin{array}{cccc}|&|&&|\\\vec{v_1} & \vec{v_2} & \cdots& \vec{v_n}\\
|&|&&|\\\end{array}\right] \left[\begin{array}{c}x_1\\x_2\\\vdots\\x_n\end{array}\right] = 
x_1\,\vec{v_1} + x_2\,\vec{v_2} + \cdots x_n\,\vec{v_n}\ .$$






"Target problem"

"Dot product" because we write it as a dot!

Vectors are explicit showing arrows.

Matrices are explicit showing two different arrows.

Write a column vector as a column. No row vectors.




## References

- Daniel T. Kaplan (2013) "Calculus and Statistics" *AMSTAT NEWS*, [link](https://magazine.amstat.org/blog/2013/07/01/calculus-and-statistics/)

- Lock 5

- George Cobb, Ptolemaic Curriculum

- Daniel T. Kaplan (2022) *MOSAIC Calculus*

- Howard Anton and Robert Busby (2003) *Contemporary Linear Algebra* Wiley

- David Lay, Steven Lay, Judi McDonald (2016) *Linear Algebra and Its Applications* (5/e), Pearson 

- Thomas McMahon and John Tyler Bonner (1983) *On Size and Life* New York: Scientific American Library, pp. 60-61
